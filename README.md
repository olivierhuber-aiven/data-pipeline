# An Aiven data pipeline with Kafka, Flink, OpenSearch and BigQuery
This material can be used as a demo and/or hands-on workshop to demonstrate in a very short time many Aiven capabilities in a smooth compelling storytelling

The full demo can take up to ~30mn but many features can be omitted to reduce this time to ~10mn if needed

The general architecture that this demo is demonstrating:

![General Architecture](images/general-architecture.png)

The concrete services that are going to be used:

![Service Architecture](images/service-architecture.png)

The Google BigQuery public eCommerce clothing fictitious data:

![Google BigQuery public data](images/bq-public-data.png)

[Synthetic eCommerce and Digital Marketing dataset](https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce)

üé• [The full 30mn recorded video (with chapters) that demonstrate all of this](https://video.aiven.io/watch/EDTGU4NXhYRcaHqTdGpkgS?)

üìä [The slides that support this hands-on](https://docs.google.com/presentation/d/1plMjAsLd5FfGJOJm6LiBPvBCDv3l1IPSGAolPG4f7C8/edit?usp=sharing)

# Steps
## Main demo ‚è±Ô∏è ~10mn
- Start from the aiven.io main webpage and point out the Get started button
- Create a Kafka **üí° rename the service simply "kafka", "os", "flink"**
  - Go through the Overview tab (explain each config)
    - Activate Rest, Schema & Connector
- Create an OpenSearch **üí° pick version 1 to be able to bump it later on**
- Setup avn CLI
- Create Kafka topics, send data `python main.py` and fetch messages from the console
- Create Kafka BigQuery connector **‚è≥ it's gonna take some time so move on to the next connector**
- Create Kafka OpenSearch connector and Query the data in OS right away
- Query data in BQ (*data should be there by now*), and export to Looker (just 1 click)
## Optional ‚è±Ô∏è ~10mn
- Integrate Kafka metrics to InfluxDB+Grafana **‚è≥ it's gonna take some time so move on to the next integration**
- Integrate Kafka logs to the running OS service and demo Query the logs right away
- Bump OS version (while logs are flowing to demonstrate the zero downtime)
- Query the metrics in Grafana (*data should be there by now*)
  - If Grafana is still not up by that time, **üí° Show the Kafka Service ‚ÄúQuick connect‚Äù button**
## Optional ‚è±Ô∏è ~10mn
- Introduce Flink (with the Carbon Footprint new field) to finish that off ü´≥üé§

# Prerequisites
- Have the üé• video ready in case there is a problem (internet, aiven console, ...)
- Set your computer & smartphone on ‚Äúdo not disturb‚Äù (and turn the sound off)
- Google BigQuery is already authenticated with the dataset emptied
- Aiven console already authenticated and the project is emptied of all services
- All Windows are Minimized except:
  - A Chrome tab with the slides
  - A Chrome tab on aiven.io
  - A Chrome tab on the Aiven BigQuery Google Console
  - A Terminal (with a big font) already at the right $path
    - Another terminal tab with the BigQuery JSON keyfile ready to be `|pbcopy`

# Cheatsheet (have this displayed on another screen or printed out on paper while you demo)
```
pip install aiven-client
avn project details && avn project switch data-pipeline
avn service list
avn user login --token
avn service user-creds-download --project data-pipeline --username avnadmin kafka
pip install -r requirements.txt
```

# Files
- `main.py` - *main file to run with python after setting the Kafka URI and Schema Registry URI*
- `requirements.txt` - *project Python dependencies*
- `order_microservice.py` - *make orders*
- `product_microservice.py` - *make products*
- `order_schema.py` - *Order Avro schema*
- `product_schema.py` - *Product Avro schema*
- `products.json` - *fictious clothing products generated from products.csv (dataset) with csv_to_json.py*
- `csv_to_json.py` - *converts products.csv file (that was present in the fictious dataset) to json*

# Todo
- [ ] Add the flink demo to the hands-on slides
- [ ] Create the Terraform script
